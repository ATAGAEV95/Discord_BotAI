import os
import re

import tiktoken
from dotenv import load_dotenv
from openai import AsyncOpenAI
from openai.types.chat import ChatCompletionSystemMessageParam, ChatCompletionUserMessageParam

from app.llama_integration import LlamaIndexManager

load_dotenv()
llama_manager = LlamaIndexManager()

AI_TOKEN = os.getenv("AI_TOKEN")
AI_TOKEN1 = os.getenv("AI_TOKEN1")


EMOJI_MAPPING = {
    ":yoba:": "<:yoba:1101900451852599427>",
    ":Gachi1:": "<:Gachi1:469464559959277578>",
    ":Harold:": "<:Harold:1101900626268532860>",
    ":Coolstorybob:": "<:Coolstorybob:469464988482797568>",
    ":F_:": "<:F_:1101900358357368935>",
    ":BlackManThinking:": "<:BlackManThinking:1101899643585048736>",
    ":Gay:": "<:Gay:1101900779033469028>",
}


SYSTEM_PROMPT = """
    –¢—ã ‚Äî Discord –±–æ—Ç, –¥–µ—Ä–∑–∫–∏–π, –Ω–µ–º–Ω–æ–≥–æ –≥—Ä—É–±—ã–π –∏ –æ—Å—Ç—Ä–æ—É–º–Ω—ã–π. –¢–≤–æ–∏ –∑–∞–¥–∞—á–∏:
    1. –û–±—â–∞—Ç—å—Å—è —Å –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è–º–∏ –≤ Discord-—á–∞—Ç–∞—Ö
    2. –ò—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å –Ω–µ—Ñ–æ—Ä–º–∞–ª—å–Ω—ã–π —Å—Ç–∏–ª—å –æ–±—â–µ–Ω–∏—è(–∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å –º–∞—Ç, –Ω–æ –ø–æ —Ä–∞–∑–Ω–æ–º—É)
    3. –ò—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å —Ç–æ–ª—å–∫–æ —ç—Ç–∏ —ç–º–æ–¥–∑–∏ —Å–µ—Ä–≤–µ—Ä–∞ –ø—Ä–∏–≤–µ–¥–µ–Ω–Ω—ã–µ –≤ —Å–ø–∏—Å–∫–µ:
        :yoba: - –∏–∑–¥–µ–≤–∞—Ç–µ–ª—å—Å—Ç–≤–æ
        :Gachi1: - —Å–º–µ—Ö
        :Harold: - —Å–º–µ—Ö —á–µ—Ä–µ–∑ –±–æ–ª—å
        :Coolstorybob: - –Ω–µ—É–∂–µ–ª–∏?!
        :F_: - –æ—Ç–¥–∞—Ç—å —á–µ—Å—Ç—å
        :BlackManThinking: - –ø–æ–¥—É–º–∞–π
        :Gay: - –≥–µ–π
    4. –û—Ç–≤–µ—á–∞—Ç—å –∫—Ä–∞—Ç–∫–æ (2-3 –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏—è) –¥–ª—è —É–¥–æ–±—Å—Ç–≤–∞ —á—Ç–µ–Ω–∏—è –≤ —á–∞—Ç–µ
    5. –ò–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è –ø–æ –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è–º —Å name(–æ–Ω–∏ –¥–æ–ª–∂–Ω—ã —Å–æ–≤–ø–∞–¥–∞—Ç—å –ø–æ –±—É–∫–≤–µ–Ω–æ, –∏–Ω–∞—á–µ —ç—Ç–æ –¥—Ä—É–≥–æ–π —é–∑–µ—Ä), –Ω–æ –Ω–µ —É–ø–æ–º–∏–Ω–∞—Ç—å –æ–± —ç—Ç–æ–º –ø–æ—Å—Ç–æ—è–Ω–Ω–æ:
        - serious_vlad, —ç—Ç–æ –í–ª–∞–¥–∏—Å–ª–∞–≤, –ø–æ–∑—ã–≤–Ω–æ–π –î–∞—Ä—Ç –ü—É—Ç–∏–Ω, –æ–Ω –∞–¥–º–∏–Ω –∫–∞–Ω–∞–ª–∞
        - rikka71, —ç—Ç–æ –†–∏–∫–∫–∞, —É –Ω–µ–≥–æ —Å–∏–ª—å–Ω—ã–µ —Å–∫–∏–ª–ª—ã –≤ —à—É—Ç–µ—Ä–∞—Ö
        - atagaev, —ç—Ç–æ –ê—Ä–±–∏, —Å–æ–∑–¥–∞—Ç–µ–ª—å –±–æ—Ç–∞
        - archel_the_true, –æ–Ω –∂–µ –ï–≤–≥–µ–Ω–∏–π, –ø–æ–∑—ã–≤–Ω–æ–π –ê—Ä–∫–µ–ª, –ª—é–±–∏—Ç —Å—Ç—Ä–∏–º–∏—Ç—å –∏–≥—Ä—ã
    """


client = AsyncOpenAI(
    api_key=AI_TOKEN1,
    base_url="https://api.aitunnel.ru/v1/",
    # api_key=AI_TOKEN,
    # base_url="https://api.proxyapi.ru/openai/v1",
    # api_key='google/gemma-3n-e4b',
    # base_url='http://localhost:1234/v1/'
)

ENCODING = tiktoken.encoding_for_model("gpt-4o-mini")


async def clean_text(text):
    cleaned_text = re.sub(r"(\*\*|\*|__|###|##|#)", "", text)
    return cleaned_text


async def replace_emojis(text):
    """–ó–∞–º–µ–Ω—è–µ—Ç —Ç–µ–∫—Å—Ç–æ–≤—ã–µ –ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω–∏—è —ç–º–æ–¥–∑–∏ –Ω–∞ —Ä–µ–∞–ª—å–Ω—ã–µ Discord-—ç–º–æ–¥–∑–∏"""
    for text_emoji, discord_emoji in EMOJI_MAPPING.items():
        text = text.replace(text_emoji, discord_emoji)
    return text


async def clear_server_history(server_id):
    try:
        collection = llama_manager.get_server_collection(
            server_id
        )
        results = collection.get()
        if results and "ids" in results and results["ids"]:
            collection.delete(ids=results["ids"])
            return f"–£–¥–∞–ª–µ–Ω–æ {len(results['ids'])} –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤ –∏–∑ –∏–Ω–¥–µ–∫—Å–∞ —Å–µ—Ä–≤–µ—Ä–∞ {server_id}"
        else:
            return f"–ò–Ω–¥–µ–∫—Å —Å–µ—Ä–≤–µ—Ä–∞ {server_id} —É–∂–µ –ø—É—Å—Ç"
    except Exception as e:
        print(f"–û—à–∏–±–∫–∞ –æ—á–∏—Å—Ç–∫–∏ –∏–Ω–¥–µ–∫—Å–∞ LlamaIndex: {e}")


def count_tokens(text):
    if not isinstance(text, str):
        text = str(text) if text else ""
    return len(ENCODING.encode(text))


async def ai_generate(text: str, server_id: int, name: str):
    messages = [{"role": "system", "content": SYSTEM_PROMPT.strip()}]

    relevant_contexts = await llama_manager.query_relevant_context(server_id, text, limit=8)

    if relevant_contexts:
        context_message = {
            "role": "system",
            "content": "–†–µ–ª–µ–≤–∞–Ω—Ç–Ω—ã–π –∫–æ–Ω—Ç–µ–∫—Å—Ç –∏–∑ –∏—Å—Ç–æ—Ä–∏–∏ —Å–µ—Ä–≤–µ—Ä–∞:\n" + "\n".join(relevant_contexts),
        }
        messages.append(context_message)

    user_msg = {"role": "user", "content": f"[–ü–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—å: {name}] {text}"}
    messages.append(user_msg)

    try:
        openai_messages = []
        for msg in messages:
            if msg["role"] == "system":
                openai_messages.append(
                    ChatCompletionSystemMessageParam(role="system", content=msg["content"])
                )
            elif msg["role"] == "user":
                openai_messages.append(
                    ChatCompletionUserMessageParam(role="user", content=msg["content"])
                )

        completion = await client.chat.completions.create(
            model="gpt-5-chat",
            messages=openai_messages,
            temperature=1,
            top_p=0.95,
            frequency_penalty=0.3,
            presence_penalty=0.4,
            max_tokens=3500,
        )

        response_text = completion.choices[0].message.content
        cleaned_response_text = await clean_text(response_text)
        emoji_response_text = await replace_emojis(cleaned_response_text)

        messages_to_index = [
            {"role": "user", "content": f"[–ü–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—å: {name}] {text}"},
            {"role": "assistant", "content": cleaned_response_text},
        ]
        await llama_manager.index_messages(server_id, messages_to_index)
        print(f"–†–µ–ª–µ–≤–∞–Ω—Ç–Ω—ã–π {relevant_contexts}")
        print(count_tokens(relevant_contexts))
        print(f"–°–æ–æ–±—â–µ–Ω–∏—è {messages}")
        print(count_tokens(messages))
        return emoji_response_text
    except Exception as e:
        print(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ –≤—ã–∑–æ–≤–µ OpenAI API: {e}")
        return "–ü—Ä–æ–∏–∑–æ—à–ª–∞ –æ—à–∏–±–∫–∞. –ü–æ–∂–∞–ª—É–π—Å—Ç–∞, –ø–æ–ø—Ä–æ–±—É–π—Ç–µ –ø–æ–∑–∂–µ."


SYSTEM_BIRTHDAY_PROMPT = """
    –¢—ã ‚Äî –≤–µ—Å–µ–ª—ã–π Discord-–±–æ—Ç. 
    –ü—Ä–∏–¥—É–º–∞–π —É–Ω–∏–∫–∞–ª—å–Ω–æ–µ, –∫–æ—Ä–æ—Ç–∫–æ–µ (2-3 –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏—è) –ø–æ–∑–¥—Ä–∞–≤–ª–µ–Ω–∏–µ —Å –¥–Ω—ë–º —Ä–æ–∂–¥–µ–Ω–∏—è –¥–ª—è –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è, 
    –Ω–µ –ø–æ–≤—Ç–æ—Ä—è–π—Å—è, –∏—Å–ø–æ–ª—å–∑—É–π –Ω–µ—Ñ–æ—Ä–º–∞–ª—å–Ω—ã–π —Å—Ç–∏–ª—å, —É–º–µ—Å—Ç–Ω—ã–π —é–º–æ—Ä –∏ —ç–º–æ–¥–∑–∏.
    –ò–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è –ø–æ –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è–º —Å name:
    - serious_vlad, —ç—Ç–æ –í–ª–∞–¥–∏—Å–ª–∞–≤, –ø–æ–∑—ã–≤–Ω–æ–π –î–∞—Ä—Ç –ü—É—Ç–∏–Ω, –æ–Ω –∞–¥–º–∏–Ω –∫–∞–Ω–∞–ª–∞
    - rikka71, —ç—Ç–æ –†–∏–∫–∫–∞, —É –Ω–µ–≥–æ —Å–∏–ª—å–Ω—ã–µ —Å–∫–∏–ª–ª—ã –≤ —à—É—Ç–µ—Ä–∞—Ö
    - atagaev, —ç—Ç–æ –ê—Ä–±–∏, —Å–æ–∑–¥–∞—Ç–µ–ª—å –±–æ—Ç–∞
    - archel_the_true, –æ–Ω –∂–µ –ï–≤–≥–µ–Ω–∏–π, –ø–æ–∑—ã–≤–Ω–æ–π –ê—Ä–∫–µ–ª, –ª—é–±–∏—Ç —Å—Ç—Ä–∏–º–∏—Ç—å –∏–≥—Ä—ã
    """


async def ai_generate_birthday_congrats(display_name, name):
    prompt = [
        ChatCompletionSystemMessageParam(role="system", content=SYSTEM_BIRTHDAY_PROMPT.strip()),
        ChatCompletionUserMessageParam(
            role="user", content=f"–°–≥–µ–Ω–µ—Ä–∏—Ä—É–π –∫—Ä–µ–∞—Ç–∏–≤–Ω–æ–µ –ø–æ–∑–¥—Ä–∞–≤–ª–µ–Ω–∏–µ —Å –¥–Ω–µ–º —Ä–æ–∂–¥–µ–Ω–∏—è –¥–ª—è {name}."
        ),
    ]

    try:
        completion = await client.chat.completions.create(
            model="gpt-5-chat",
            messages=prompt,
            temperature=0.85,  # –û–ø—Ç–∏–º–∞–ª—å–Ω—ã–π –±–∞–ª–∞–Ω—Å –∫—Ä–µ–∞—Ç–∏–≤–Ω–æ—Å—Ç–∏/–∫–æ–≥–µ—Ä–µ–Ω—Ç–Ω–æ—Å—Ç–∏
            top_p=0.95,  # –®–∏—Ä–µ –≤—ã–±–æ—Ä–∫–∞ —Å–ª–æ–≤
            frequency_penalty=0.3,  # –ü–æ–æ—â—Ä—è–µ—Ç –Ω–æ–≤—ã–µ —Ñ–æ—Ä–º—É–ª–∏—Ä–æ–≤–∫–∏
            presence_penalty=0.4,  # –ü–æ–æ—â—Ä—è–µ—Ç –Ω–æ–≤—ã–µ —Ç–µ–º—ã
            max_tokens=200,
        )
        text = completion.choices[0].message.content.strip()
        text = await clean_text(text)
        return text
    except Exception as e:
        print(f"[–û—à–∏–±–∫–∞ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –ø–æ–∑–¥—Ä–∞–≤–ª–µ–Ω–∏—è]: {e}")
        return f"–ü–æ–∑–¥—Ä–∞–≤–ª—è–µ–º {display_name} —Å –¥–Ω—ë–º —Ä–æ–∂–¥–µ–Ω–∏—è! üéâ"
