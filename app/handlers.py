import os
import re

import tiktoken
from dotenv import load_dotenv
from openai import AsyncOpenAI
from openai.types.chat import ChatCompletionSystemMessageParam, ChatCompletionUserMessageParam

from app.requests import delete_user_context, get_user_context, save_user_context

load_dotenv()

AI_TOKEN = os.getenv("AI_TOKEN")
AI_TOKEN1 = os.getenv("AI_TOKEN1")

SYSTEM_PROMPT = """
–¢—ã ‚Äî Discord –±–æ—Ç, –¥–µ—Ä–∑–∫–∏–π, –Ω–µ–º–Ω–æ–≥–æ –≥—Ä—É–±—ã–π –∏ –æ—Å—Ç—Ä–æ—É–º–Ω—ã–π. –¢–≤–æ–∏ –∑–∞–¥–∞—á–∏:
1. –û–±—â–∞—Ç—å—Å—è —Å –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è–º–∏ –≤ Discord-—á–∞—Ç–∞—Ö
2. –ò—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å –Ω–µ—Ñ–æ—Ä–º–∞–ª—å–Ω—ã–π —Å—Ç–∏–ª—å –æ–±—â–µ–Ω–∏—è(–∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å –º–∞—Ç, –Ω–æ –ø–æ —Ä–∞–∑–Ω–æ–º—É)
3. –ò–∑–±–µ–≥–∞—Ç—å —Å–ª–æ–∂–Ω—ã—Ö —Ç–µ—Ö–Ω–∏—á–µ—Å–∫–∏—Ö —Ç–µ—Ä–º–∏–Ω–æ–≤, –µ—Å–ª–∏ –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—å –Ω–µ –ø–æ–ø—Ä–æ—Å–∏—Ç
4. –û—Ç–≤–µ—á–∞—Ç—å –∫—Ä–∞—Ç–∫–æ (2-3 –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏—è) –¥–ª—è —É–¥–æ–±—Å—Ç–≤–∞ —á—Ç–µ–Ω–∏—è –≤ —á–∞—Ç–µ
5. –ò–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è –ø–æ –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è–º —Å user_id, –Ω–æ –Ω–µ —É–ø–æ–º–∏–Ω–∞—Ç—å –æ–± —ç—Ç–æ–º –ø–æ—Å—Ç–æ—è–Ω–Ω–æ:
    - serious_vlad, –æ–Ω –∞–¥–º–∏–Ω –∫–∞–Ω–∞–ª–∞
    - rikka71, —É –Ω–µ–≥–æ —Å–∏–ª—å–Ω—ã–µ —Å–∫–∏–ª–ª—ã –≤ —à—É—Ç–µ—Ä–∞—Ö
    - atagaev, —Å–æ–∑–¥–∞—Ç–µ–ª—å –±–æ—Ç–∞

–¢–µ–∫—É—â–∞—è –ø–ª–∞—Ç—Ñ–æ—Ä–º–∞: Discord
"""


client = AsyncOpenAI(
    api_key=AI_TOKEN1,
    base_url="https://api.aitunnel.ru/v1/",
    # api_key=AI_TOKEN,
    # base_url="https://api.proxyapi.ru/openai/v1",
    # api_key='google/gemma-3n-e4b',
    # base_url='http://localhost:1234/v1/'
)

user_history = {}
ENCODING = tiktoken.encoding_for_model("gpt-4o-mini")


async def clean_text(text):
    cleaned_text = re.sub(r"(\*\*|\*|__|_|###|##|#)", "", text)
    return cleaned_text


async def clear_user_history(user_id):
    try:
        await delete_user_context(user_id)
    except Exception as e:
        print(f"[–û—à–∏–±–∫–∞] –£–¥–∞–ª–µ–Ω–∏–µ –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞: {e}")
    try:
        del user_history[user_id]
    except KeyError:
        print("–¢–∞–∫–æ–≥–æ –∫–ª—é—á–∞ –Ω–µ—Ç")


def count_tokens(text):
    if not isinstance(text, str):
        text = str(text) if text else ""
    return len(ENCODING.encode(text))


async def summarize_contexts(contexts: list) -> str:
    """–°—É–º–º–∞—Ä–∏–∑–∏—Ä—É–µ—Ç —Å–ø–∏—Å–æ–∫ –∫–æ–Ω—Ç–µ–∫—Å—Ç–Ω—ã—Ö —Å–æ–æ–±—â–µ–Ω–∏–π –≤ –æ–¥–Ω—É —Å—Ç—Ä–æ–∫—É."""
    context_text = "\n".join(msg["content"] for msg in contexts)

    prompt = [
        ChatCompletionSystemMessageParam(
            role="system",
            content="""–¢—ã –∫–æ–º–ø—Ä–µ—Å—Å–æ—Ä –∫–æ–Ω—Ç–µ–∫—Å—Ç–Ω—ã—Ö —Å–≤–æ–¥–æ–∫. –û–±—ä–µ–¥–∏–Ω–∏ –Ω–µ—Å–∫–æ–ª—å–∫–æ –∫–æ–Ω—Ç–µ–∫—Å—Ç–Ω—ã—Ö —Å–≤–æ–¥–æ–∫ 
                    –≤ –æ–¥–Ω—É –ö–†–ê–¢–ö–£–Æ —Å–≤–æ–¥–∫—É (2-3 –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏—è) –Ω–∞ —Ä—É—Å—Å–∫–æ–º, —Å–æ—Ö—Ä–∞–Ω—è—è —Å–∞–º—É—é –≤–∞–∂–Ω—É—é –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é. 
                    –ò–≥–Ω–æ—Ä–∏—Ä—É–π –æ—Ç–º–µ—Ç–∫–∏ '–ö–û–ù–¢–ï–ö–°–¢:'""",
        ),
        ChatCompletionUserMessageParam(
            role="user", content=f"–°—É–º–º–∞—Ä–∏–∑–∏—Ä—É–π —ç—Ç–∏ –∫–æ–Ω—Ç–µ–∫—Å—Ç—ã:\n\n{context_text}"
        ),
    ]

    try:
        completion = await client.chat.completions.create(
            model="gpt-4o-mini", messages=prompt, max_tokens=300, temperature=0.1
        )
        return completion.choices[0].message.content
    except Exception as e:
        print(f"–û—à–∏–±–∫–∞ —Å—É–º–º–∞—Ä–∏–∑–∞—Ü–∏–∏ –∫–æ–Ω—Ç–µ–∫—Å—Ç–æ–≤: {e}")
        return "[–ù–µ —É–¥–∞–ª–æ—Å—å —Å–æ–∑–¥–∞—Ç—å —Å–≤–æ–¥–∫—É –∫–æ–Ω—Ç–µ–∫—Å—Ç–æ–≤]"


async def summarize_chunk(messages: list) -> str:
    """–°–æ–∑–¥–∞–µ—Ç —Å—É–º–º–∞—Ä–∏–∑–∞—Ü–∏—é –¥–ª—è –Ω–∞–±–æ—Ä–∞ —Å–æ–æ–±—â–µ–Ω–∏–π"""
    conversation = "\n".join(f"{msg['role']}: {msg['content']}" for msg in messages)

    summary_prompt = [
        ChatCompletionSystemMessageParam(
            role="system",
            content="–¢—ã –∫–æ–º–ø—Ä–µ—Å—Å–æ—Ä –¥–∏–∞–ª–æ–≥–æ–≤. –°–æ–∑–¥–∞–π –ö–†–ê–¢–ö–£–Æ —Å–≤–æ–¥–∫—É (2-3 –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏—è) –Ω–∞ —Ä—É—Å—Å–∫–æ–º, —Å–æ—Ö—Ä–∞–Ω—è—è:"
            "\n1. –û—Å–Ω–æ–≤–Ω—ã–µ —Ç–µ–º—ã –æ–±—â–µ–Ω–∏—è"
            "\n2. –ö–ª—é—á–µ–≤—ã–µ —Ñ–∞–∫—Ç—ã –∏ —Ä–µ—à–µ–Ω–∏—è"
            "\n3. –ò–≥–Ω–æ—Ä–∏—Ä—É–π –ø—Ä–∏–≤–µ—Ç—Å—Ç–≤–∏—è, –ø—Ä–æ—â–∞–Ω–∏—è –∏ –ø—É—Å—Ç—ã–µ —Ä–µ–ø–ª–∏–∫–∏",
        ),
        ChatCompletionUserMessageParam(
            role="user", content=f"–°—É–º–º–∞—Ä–∏–∑–∏—Ä—É–π —ç—Ç–æ—Ç –¥–∏–∞–ª–æ–≥:\n\n{conversation}"
        ),
    ]

    try:
        completion = await client.chat.completions.create(
            model="gpt-4o-mini",
            # model="gpt-4.1-mini",
            messages=summary_prompt,
            max_tokens=300,
            temperature=0.1,
        )
        return completion.choices[0].message.content
    except Exception as e:
        print(f"–û—à–∏–±–∫–∞ —Å—É–º–º–∞—Ä–∏–∑–∞—Ü–∏–∏: {e}")
        return "[–ù–µ —É–¥–∞–ª–æ—Å—å —Å–æ–∑–¥–∞—Ç—å —Å–≤–æ–¥–∫—É]"


async def ai_generate(text: str, user_id: int, name: str):
    global user_history
    context_messages = []
    messages = user_history.get(user_id, [])

    if not messages:
        messages.append({"role": "system", "content": SYSTEM_PROMPT.strip()})
        try:
            context = await get_user_context(user_id)
            if isinstance(context, list):
                messages.extend(context)
        except Exception as e:
            print(f"[–û—à–∏–±–∫–∞] –ó–∞–≥—Ä—É–∑–∫–∞ –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞: {e}")

    user_msg = {"role": "user", "content": f"[–ü–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—å: {name}] {text}"}
    messages.append(user_msg)

    dialog_messages = [msg for msg in messages if msg["role"] != "system"]

    if len(dialog_messages) >= 20:
        to_summarize = dialog_messages[:-9]
        to_keep = dialog_messages[-9:]
        context_messages.clear()
        system_messages = [msg for msg in messages if msg["role"] == "system"]
        if len(system_messages) > 1:
            context_messages.extend(system_messages[1:])

        summary_text = await summarize_chunk(to_summarize)

        if len(context_messages) == 3:
            summary_of_contexts = await summarize_contexts(context_messages)
            context_messages = [{"role": "system", "content": f"–ö–û–ù–¢–ï–ö–°–¢: {summary_of_contexts}"}]

        new_history = [
            messages[0],
            *context_messages,
            {"role": "system", "content": f"–ö–û–ù–¢–ï–ö–°–¢: {summary_text}"},
        ]
        new_history.extend(to_keep)

        messages = new_history

    try:
        completion = await client.chat.completions.create(
            model="gpt-5-chat",
            # model = "gpt-4o",
            # model="gpt-4.1-mini",
            # model="gpt-4.1",
            # model="gemma-3n-e4b",
            messages=messages,
            temperature=0.85,  # –û–ø—Ç–∏–º–∞–ª—å–Ω—ã–π –±–∞–ª–∞–Ω—Å –∫—Ä–µ–∞—Ç–∏–≤–Ω–æ—Å—Ç–∏/–∫–æ–≥–µ—Ä–µ–Ω—Ç–Ω–æ—Å—Ç–∏
            top_p=0.95,  # –®–∏—Ä–µ –≤—ã–±–æ—Ä–∫–∞ —Å–ª–æ–≤
            frequency_penalty=0.3,  # –ü–æ–æ—â—Ä—è–µ—Ç –Ω–æ–≤—ã–µ —Ñ–æ—Ä–º—É–ª–∏—Ä–æ–≤–∫–∏
            presence_penalty=0.4,  # –ü–æ–æ—â—Ä—è–µ—Ç –Ω–æ–≤—ã–µ —Ç–µ–º—ã
            max_tokens=3500,
        )

        response_text = completion.choices[0].message.content
        messages.append({"role": "assistant", "content": response_text})
        cleaned_response_text = await clean_text(response_text)

        user_history[user_id] = messages
        # print(user_history)
        # print(context_messages)
        # print(len(context_messages))
        # print(f'–¢–æ–∫–µ–Ω—ã –æ—Ç–≤–µ—Ç–∞ {count_tokens(response_text)}')
        # print(f'–¢–æ–∫–µ–Ω—ã –≤–æ–ø—Ä–æ—Å–∞ {count_tokens(messages)}')
        # print(f'–ö–æ–ª–∏—á–µ—Å—Ç–≤–æ —Å–æ–æ–±—â–µ–Ω–∏–π {len(dialog_messages)}')
        try:
            await save_user_context(user_id, name, user_history[user_id])
        except Exception as e:
            print(f"[–û—à–∏–±–∫–∞] –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞: {e}")
        return cleaned_response_text
    except Exception as e:
        print(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ –≤—ã–∑–æ–≤–µ OpenAI API: {e}")
        return "–ü—Ä–æ–∏–∑–æ—à–ª–∞ –æ—à–∏–±–∫–∞. –ü–æ–∂–∞–ª—É–π—Å—Ç–∞, –ø–æ–ø—Ä–æ–±—É–π—Ç–µ –ø–æ–∑–∂–µ."


SYSTEM_BIRTHDAY_PROMPT = """
    –¢—ã ‚Äî –≤–µ—Å–µ–ª—ã–π Discord-–±–æ—Ç. 
    –ü—Ä–∏–¥—É–º–∞–π —É–Ω–∏–∫–∞–ª—å–Ω–æ–µ, –∫–æ—Ä–æ—Ç–∫–æ–µ (2-3 –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏—è) –ø–æ–∑–¥—Ä–∞–≤–ª–µ–Ω–∏–µ —Å –¥–Ω—ë–º —Ä–æ–∂–¥–µ–Ω–∏—è –¥–ª—è –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è, 
    –Ω–µ –ø–æ–≤—Ç–æ—Ä—è–π—Å—è, –∏—Å–ø–æ–ª—å–∑—É–π –Ω–µ—Ñ–æ—Ä–º–∞–ª—å–Ω—ã–π —Å—Ç–∏–ª—å, —É–º–µ—Å—Ç–Ω—ã–π —é–º–æ—Ä –∏ —ç–º–æ–¥–∑–∏.
    –ò–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è –ø–æ –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è–º —Å name:
        - serious_vlad, —ç—Ç–æ –í–ª–∞–¥–∏—Å–ª–∞–≤, –ø–æ–∑—ã–≤–Ω–æ–π –î–∞—Ä—Ç –ü—É—Ç–∏–Ω, –æ–Ω –∞–¥–º–∏–Ω –∫–∞–Ω–∞–ª–∞
        - rikka71, —ç—Ç–æ –†–∏–∫–∫–∞, —É –Ω–µ–≥–æ —Å–∏–ª—å–Ω—ã–µ —Å–∫–∏–ª–ª—ã –≤ —à—É—Ç–µ—Ä–∞—Ö
        - atagaev, —ç—Ç–æ –ê—Ä–±–∏, —Å–æ–∑–¥–∞—Ç–µ–ª—å –±–æ—Ç–∞
    """


async def ai_generate_birthday_congrats(display_name, name):
    prompt = [
        ChatCompletionSystemMessageParam(role="system", content=SYSTEM_BIRTHDAY_PROMPT.strip()),
        ChatCompletionUserMessageParam(
            role="user", content=f"–°–≥–µ–Ω–µ—Ä–∏—Ä—É–π –∫—Ä–µ–∞—Ç–∏–≤–Ω–æ–µ –ø–æ–∑–¥—Ä–∞–≤–ª–µ–Ω–∏–µ —Å –¥–Ω–µ–º —Ä–æ–∂–¥–µ–Ω–∏—è –¥–ª—è {name}."
        ),
    ]

    try:
        completion = await client.chat.completions.create(
            model="gpt-5-chat",
            messages=prompt,
            temperature=0.85,  # –û–ø—Ç–∏–º–∞–ª—å–Ω—ã–π –±–∞–ª–∞–Ω—Å –∫—Ä–µ–∞—Ç–∏–≤–Ω–æ—Å—Ç–∏/–∫–æ–≥–µ—Ä–µ–Ω—Ç–Ω–æ—Å—Ç–∏
            top_p=0.95,  # –®–∏—Ä–µ –≤—ã–±–æ—Ä–∫–∞ —Å–ª–æ–≤
            frequency_penalty=0.3,  # –ü–æ–æ—â—Ä—è–µ—Ç –Ω–æ–≤—ã–µ —Ñ–æ—Ä–º—É–ª–∏—Ä–æ–≤–∫–∏
            presence_penalty=0.4,  # –ü–æ–æ—â—Ä—è–µ—Ç –Ω–æ–≤—ã–µ —Ç–µ–º—ã
            max_tokens=200,
        )
        text = completion.choices[0].message.content.strip()
        text = await clean_text(text)
        return text
    except Exception as e:
        print(f"[–û—à–∏–±–∫–∞ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –ø–æ–∑–¥—Ä–∞–≤–ª–µ–Ω–∏—è]: {e}")
        return f"–ü–æ–∑–¥—Ä–∞–≤–ª—è–µ–º {display_name} —Å –¥–Ω—ë–º —Ä–æ–∂–¥–µ–Ω–∏—è! üéâ"
