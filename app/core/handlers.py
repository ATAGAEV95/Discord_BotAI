import json
import os

from dotenv import load_dotenv
from openai import AsyncOpenAI
from openai.types.chat import ChatCompletionSystemMessageParam, ChatCompletionUserMessageParam
from mcp import ClientSession, StdioServerParameters
from mcp.client.stdio import stdio_client

from app.services.llama_integration import LlamaIndexManager
from app.tools.prompt import SYSTEM_BIRTHDAY_PROMPT, USER_DESCRIPTIONS, WEATHER_PROMPT
from app.tools.utils import (
    clean_text,
    count_tokens,
    enrich_users_context,
    replace_emojis,
    user_prompt, convert_mcp_tools_to_openai,
)

load_dotenv()
llama_manager = LlamaIndexManager()

AI_TOKEN = os.getenv("AI_TOKEN")
AI_TOKEN1 = os.getenv("AI_TOKEN1")
AI_TOKEN_POLZA = os.getenv("AI_TOKEN_POLZA")


client = AsyncOpenAI(
    api_key=AI_TOKEN_POLZA,
    # base_url="https://api.aitunnel.ru/v1/",
    base_url="https://api.polza.ai/api/v1",
    # api_key=AI_TOKEN,
    # base_url="https://api.proxyapi.ru/openai/v1",
    # api_key='google/gemma-3n-e4b',
    # base_url='http://localhost:1234/v1/'
)


async def clear_server_history(server_id):
    """–û—á–∏—â–∞–µ—Ç –∏—Å—Ç–æ—Ä–∏—é —Å–æ–æ–±—â–µ–Ω–∏–π —Å–µ—Ä–≤–µ—Ä–∞ –≤ –∏–Ω–¥–µ–∫—Å–µ LlamaIndex,
    –æ—Å—Ç–∞–≤–ª—è—è —Ç–æ–ª—å–∫–æ –¥–æ–∫—É–º–µ–Ω—Ç—ã —Ç–∏–ø–∞ 'server_users'.
    """
    try:
        collection = llama_manager.get_server_collection(server_id)
        results = collection.get()
        if results and "ids" in results and results["ids"]:
            ids_to_delete = []
            metadatas = results.get("metadatas", [])

            for i, metadata in enumerate(metadatas):
                if metadata.get("document_type") != "server_users":
                    ids_to_delete.append(results["ids"][i])

            if ids_to_delete:
                collection.delete(ids=ids_to_delete)
                return f"–£–¥–∞–ª–µ–Ω–æ {len(ids_to_delete)} –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤ –∏–∑ –∏–Ω–¥–µ–∫—Å–∞ —Å–µ—Ä–≤–µ—Ä–∞ {server_id}"
            else:
                return f"–í –∏–Ω–¥–µ–∫—Å–µ —Å–µ—Ä–≤–µ—Ä–∞ {server_id} –Ω–µ—Ç –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤ –¥–ª—è —É–¥–∞–ª–µ–Ω–∏—è (–∫—Ä–æ–º–µ —Å–ø–∏—Å–∫–∞ –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª–µ–π)"
        else:
            return f"–ò–Ω–¥–µ–∫—Å —Å–µ—Ä–≤–µ—Ä–∞ {server_id} —É–∂–µ –ø—É—Å—Ç"
    except Exception as e:
        print(f"–û—à–∏–±–∫–∞ –æ—á–∏—Å—Ç–∫–∏ –∏–Ω–¥–µ–∫—Å–∞ LlamaIndex: {e}")
        return f"–ü—Ä–æ–∏–∑–æ—à–ª–∞ –æ—à–∏–±–∫–∞ –ø—Ä–∏ –æ—á–∏—Å—Ç–∫–µ –∏–Ω–¥–µ–∫—Å–∞: {e}"


async def ai_generate(text: str, server_id: int, name: str, tool_result: str) -> str:
    """–ì–µ–Ω–µ—Ä–∏—Ä—É–µ—Ç –æ—Ç–≤–µ—Ç –æ—Ç AI –Ω–∞ –æ—Å–Ω–æ–≤–µ –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞ —Å–µ—Ä–≤–µ—Ä–∞ –∏ —Ç–µ–∫—É—â–µ–≥–æ —Å–æ–æ–±—â–µ–Ω–∏—è –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è."""
    messages = [{"role": "system", "content": user_prompt(f"{name}")}]
    relevant_contexts = await llama_manager.query_relevant_context(server_id, text, limit=16)
    relevant_contexts = enrich_users_context(relevant_contexts, USER_DESCRIPTIONS)

    if relevant_contexts:
        context_message = {
            "role": "system",
            "content": "–†–µ–ª–µ–≤–∞–Ω—Ç–Ω—ã–π –∫–æ–Ω—Ç–µ–∫—Å—Ç –∏–∑ –∏—Å—Ç–æ—Ä–∏–∏ —Å–µ—Ä–≤–µ—Ä–∞:\n" + "\n".join(relevant_contexts),
        }
        messages.append(context_message)

    if tool_result and tool_result.strip():
        tool_message = {
            "role": "system",
            "content": f"–î–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω–∞—è –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è –æ—Ç –∏–Ω—Å—Ç—Ä—É–º–µ–Ω—Ç–æ–≤: {tool_result}"
        }
        messages.append(tool_message)

    user_msg = {"role": "user", "content": f"[–ü–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—å: {name}] {text}"}
    messages.append(user_msg)

    try:
        openai_messages = []
        for msg in messages:
            if msg["role"] == "system":
                openai_messages.append(
                    ChatCompletionSystemMessageParam(role="system", content=msg["content"])
                )
            elif msg["role"] == "user":
                openai_messages.append(
                    ChatCompletionUserMessageParam(role="user", content=msg["content"])
                )

        completion = await client.chat.completions.create(
            model="gpt-5-chat",
            messages=openai_messages,
            temperature=0.8,
            top_p=0.8,
            frequency_penalty=0.1,
            presence_penalty=0.2,
            max_tokens=3500,
        )

        response_text = completion.choices[0].message.content
        cleaned_response_text = await clean_text(response_text)
        emoji_response_text = await replace_emojis(cleaned_response_text)

        messages_to_index = [
            {"role": "user", "content": f"[–ü–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—å: {name}] {text}"},
            {"role": "assistant", "content": cleaned_response_text},
        ]
        await llama_manager.index_messages(server_id, messages_to_index)
        print(f"–†–µ–ª–µ–≤–∞–Ω—Ç–Ω—ã–π {relevant_contexts}")
        print(count_tokens(relevant_contexts))
        print(f"–°–æ–æ–±—â–µ–Ω–∏—è {messages}")
        print(count_tokens(messages))
        return emoji_response_text
    except Exception as e:
        print(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ –≤—ã–∑–æ–≤–µ OpenAI API: {e}")
        return "–ü—Ä–æ–∏–∑–æ—à–ª–∞ –æ—à–∏–±–∫–∞. –ü–æ–∂–∞–ª—É–π—Å—Ç–∞, –ø–æ–ø—Ä–æ–±—É–π—Ç–µ –ø–æ–∑–∂–µ."


async def ai_generate_birthday_congrats(display_name, name):
    """–ì–µ–Ω–µ—Ä–∏—Ä—É–µ—Ç –∫—Ä–µ–∞—Ç–∏–≤–Ω–æ–µ –ø–æ–∑–¥—Ä–∞–≤–ª–µ–Ω–∏–µ —Å –¥–Ω—ë–º —Ä–æ–∂–¥–µ–Ω–∏—è –¥–ª—è –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è."""
    prompt = [
        ChatCompletionSystemMessageParam(role="system", content=SYSTEM_BIRTHDAY_PROMPT.strip()),
        ChatCompletionUserMessageParam(
            role="user", content=f"–°–≥–µ–Ω–µ—Ä–∏—Ä—É–π –∫—Ä–µ–∞—Ç–∏–≤–Ω–æ–µ –ø–æ–∑–¥—Ä–∞–≤–ª–µ–Ω–∏–µ —Å –¥–Ω–µ–º —Ä–æ–∂–¥–µ–Ω–∏—è –¥–ª—è {name}."
        ),
    ]

    try:
        completion = await client.chat.completions.create(
            model="gpt-5-chat",
            messages=prompt,
            temperature=0.8,  # –û–ø—Ç–∏–º–∞–ª—å–Ω—ã–π –±–∞–ª–∞–Ω—Å –∫—Ä–µ–∞—Ç–∏–≤–Ω–æ—Å—Ç–∏/–∫–æ–≥–µ—Ä–µ–Ω—Ç–Ω–æ—Å—Ç–∏
            top_p=0.8,  # –®–∏—Ä–µ –≤—ã–±–æ—Ä–∫–∞ —Å–ª–æ–≤
            frequency_penalty=0.1,  # –ü–æ–æ—â—Ä—è–µ—Ç –Ω–æ–≤—ã–µ —Ñ–æ—Ä–º—É–ª–∏—Ä–æ–≤–∫–∏
            presence_penalty=0.2,  # –ü–æ–æ—â—Ä—è–µ—Ç –Ω–æ–≤—ã–µ —Ç–µ–º—ã
            max_tokens=400,
        )
        text = completion.choices[0].message.content.strip()
        text = await clean_text(text)
        return text
    except Exception as e:
        print(f"[–û—à–∏–±–∫–∞ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –ø–æ–∑–¥—Ä–∞–≤–ª–µ–Ω–∏—è]: {e}")
        return f"–ü–æ–∑–¥—Ä–∞–≤–ª—è–µ–º {display_name} —Å –¥–Ω—ë–º —Ä–æ–∂–¥–µ–Ω–∏—è! üéâ"


async def check_weather_intent(text: str) -> str:
    server_params = StdioServerParameters(
        command="python",
        args=["app/mcp/server.py"],
        env=None
    )

    async with stdio_client(server_params) as (read, write):
        async with ClientSession(read, write) as session:
            await session.initialize()
            tools_list = await session.list_tools()

            openai_tools = convert_mcp_tools_to_openai(tools_list.tools)

            messages = [
                ChatCompletionSystemMessageParam(role="system", content=WEATHER_PROMPT.strip()),
                ChatCompletionUserMessageParam(
                    role="user", content=text
                ),
            ]

            final_response = await process_conversation(
                messages,
                openai_tools,
                session
            )

            return final_response


async def process_conversation(messages: list, tools: list, session: ClientSession) -> str:
    """–û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ—Ç —Ä–∞–∑–≥–æ–≤–æ—Ä —Å –≤–æ–∑–º–æ–∂–Ω—ã–º–∏ –≤—ã–∑–æ–≤–∞–º–∏ –∏–Ω—Å—Ç—Ä—É–º–µ–Ω—Ç–æ–≤."""
    response = await client.chat.completions.create(
        model="gpt-4o-mini",
        messages=messages,
        tools=tools,  # –ü–µ—Ä–µ–¥–∞–µ–º –¥–æ—Å—Ç—É–ø–Ω—ã–µ –∏–Ω—Å—Ç—Ä—É–º–µ–Ω—Ç—ã
        tool_choice="auto"  # –ú–æ–¥–µ–ª—å —Å–∞–º–∞ —Ä–µ—à–∞–µ—Ç, –Ω—É–∂–Ω—ã –ª–∏ –∏–Ω—Å—Ç—Ä—É–º–µ–Ω—Ç—ã
    )

    assistant_message = response.choices[0].message

    if not assistant_message.tool_calls:
        return assistant_message.content or "–ò–∑–≤–∏–Ω–∏—Ç–µ, —è –Ω–µ —Å–º–æ–≥ —Å—Ñ–æ—Ä–º–∏—Ä–æ–≤–∞—Ç—å –æ—Ç–≤–µ—Ç."

    for tool_call in assistant_message.tool_calls:
        function_name = tool_call.function.name
        function_args = json.loads(tool_call.function.arguments)

        try:
            result = await session.call_tool(function_name, function_args)

            if result.content:
                tool_result = result.content[0].text if result.content else "–ù–µ—Ç —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∞"
            else:
                tool_result = "–ò–Ω—Å—Ç—Ä—É–º–µ–Ω—Ç –≤—ã–ø–æ–ª–Ω–µ–Ω, –Ω–æ —Ä–µ–∑—É–ª—å—Ç–∞—Ç –ø—É—Å—Ç"

            return tool_result

        except Exception as e:
            print(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ –≤—ã–∑–æ–≤–µ –∏–Ω—Å—Ç—Ä—É–º–µ–Ω—Ç–∞: {str(e)}")

    return ''
