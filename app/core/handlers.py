import os

from dotenv import load_dotenv
from openai import AsyncOpenAI
from openai.types.chat import ChatCompletionSystemMessageParam, ChatCompletionUserMessageParam

from app.services.llama_integration import LlamaIndexManager
from app.tools.prompt import SYSTEM_BIRTHDAY_PROMPT, USER_DESCRIPTIONS
from app.tools.utils import (
    clean_text,
    count_tokens,
    enrich_users_context,
    replace_emojis,
    user_prompt,
)

load_dotenv()
llama_manager = LlamaIndexManager()

AI_TOKEN = os.getenv("AI_TOKEN")
AI_TOKEN1 = os.getenv("AI_TOKEN1")


client = AsyncOpenAI(
    api_key=AI_TOKEN1,
    base_url="https://api.aitunnel.ru/v1/",
    # api_key=AI_TOKEN,
    # base_url="https://api.proxyapi.ru/openai/v1",
    # api_key='google/gemma-3n-e4b',
    # base_url='http://localhost:1234/v1/'
)


async def clear_server_history(server_id):
    """–û—á–∏—â–∞–µ—Ç –∏—Å—Ç–æ—Ä–∏—é —Å–æ–æ–±—â–µ–Ω–∏–π —Å–µ—Ä–≤–µ—Ä–∞ –≤ –∏–Ω–¥–µ–∫—Å–µ LlamaIndex,
    –æ—Å—Ç–∞–≤–ª—è—è —Ç–æ–ª—å–∫–æ –¥–æ–∫—É–º–µ–Ω—Ç—ã —Ç–∏–ø–∞ 'server_users'."""
    try:
        collection = llama_manager.get_server_collection(server_id)
        results = collection.get()
        if results and "ids" in results and results["ids"]:
            ids_to_delete = []
            metadatas = results.get("metadatas", [])

            for i, metadata in enumerate(metadatas):
                if metadata.get("document_type") != "server_users":
                    ids_to_delete.append(results["ids"][i])

            if ids_to_delete:
                collection.delete(ids=ids_to_delete)
                return f"–£–¥–∞–ª–µ–Ω–æ {len(ids_to_delete)} –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤ –∏–∑ –∏–Ω–¥–µ–∫—Å–∞ —Å–µ—Ä–≤–µ—Ä–∞ {server_id}"
            else:
                return f"–í –∏–Ω–¥–µ–∫—Å–µ —Å–µ—Ä–≤–µ—Ä–∞ {server_id} –Ω–µ—Ç –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤ –¥–ª—è —É–¥–∞–ª–µ–Ω–∏—è (–∫—Ä–æ–º–µ —Å–ø–∏—Å–∫–∞ –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª–µ–π)"
        else:
            return f"–ò–Ω–¥–µ–∫—Å —Å–µ—Ä–≤–µ—Ä–∞ {server_id} —É–∂–µ –ø—É—Å—Ç"
    except Exception as e:
        print(f"–û—à–∏–±–∫–∞ –æ—á–∏—Å—Ç–∫–∏ –∏–Ω–¥–µ–∫—Å–∞ LlamaIndex: {e}")
        return f"–ü—Ä–æ–∏–∑–æ—à–ª–∞ –æ—à–∏–±–∫–∞ –ø—Ä–∏ –æ—á–∏—Å—Ç–∫–µ –∏–Ω–¥–µ–∫—Å–∞: {e}"


async def ai_generate(text: str, server_id: int, name: str) -> str:
    """–ì–µ–Ω–µ—Ä–∏—Ä—É–µ—Ç –æ—Ç–≤–µ—Ç –æ—Ç AI –Ω–∞ –æ—Å–Ω–æ–≤–µ –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞ —Å–µ—Ä–≤–µ—Ä–∞ –∏ —Ç–µ–∫—É—â–µ–≥–æ —Å–æ–æ–±—â–µ–Ω–∏—è –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è."""
    messages = [{"role": "system", "content": user_prompt(f"{name}")}]
    relevant_contexts = await llama_manager.query_relevant_context(server_id, text, limit=16)
    relevant_contexts = enrich_users_context(relevant_contexts, USER_DESCRIPTIONS)

    if relevant_contexts:
        context_message = {
            "role": "system",
            "content": "–†–µ–ª–µ–≤–∞–Ω—Ç–Ω—ã–π –∫–æ–Ω—Ç–µ–∫—Å—Ç –∏–∑ –∏—Å—Ç–æ—Ä–∏–∏ —Å–µ—Ä–≤–µ—Ä–∞:\n" + "\n".join(relevant_contexts),
        }
        messages.append(context_message)

    user_msg = {"role": "user", "content": f"[–ü–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—å: {name}] {text}"}
    messages.append(user_msg)

    try:
        openai_messages = []
        for msg in messages:
            if msg["role"] == "system":
                openai_messages.append(
                    ChatCompletionSystemMessageParam(role="system", content=msg["content"])
                )
            elif msg["role"] == "user":
                openai_messages.append(
                    ChatCompletionUserMessageParam(role="user", content=msg["content"])
                )

        completion = await client.chat.completions.create(
            model="gpt-5-chat",
            messages=openai_messages,
            temperature=0.8,
            top_p=0.8,
            frequency_penalty=0.1,
            presence_penalty=0.2,
            max_tokens=3500,
        )

        response_text = completion.choices[0].message.content
        cleaned_response_text = await clean_text(response_text)
        emoji_response_text = await replace_emojis(cleaned_response_text)

        messages_to_index = [
            {"role": "user", "content": f"[–ü–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—å: {name}] {text}"},
            {"role": "assistant", "content": cleaned_response_text},
        ]
        await llama_manager.index_messages(server_id, messages_to_index)
        print(f"–†–µ–ª–µ–≤–∞–Ω—Ç–Ω—ã–π {relevant_contexts}")
        print(count_tokens(relevant_contexts))
        print(f"–°–æ–æ–±—â–µ–Ω–∏—è {messages}")
        print(count_tokens(messages))
        return emoji_response_text
    except Exception as e:
        print(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ –≤—ã–∑–æ–≤–µ OpenAI API: {e}")
        return "–ü—Ä–æ–∏–∑–æ—à–ª–∞ –æ—à–∏–±–∫–∞. –ü–æ–∂–∞–ª—É–π—Å—Ç–∞, –ø–æ–ø—Ä–æ–±—É–π—Ç–µ –ø–æ–∑–∂–µ."


async def ai_generate_birthday_congrats(display_name, name):
    """–ì–µ–Ω–µ—Ä–∏—Ä—É–µ—Ç –∫—Ä–µ–∞—Ç–∏–≤–Ω–æ–µ –ø–æ–∑–¥—Ä–∞–≤–ª–µ–Ω–∏–µ —Å –¥–Ω—ë–º —Ä–æ–∂–¥–µ–Ω–∏—è –¥–ª—è –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è."""
    prompt = [
        ChatCompletionSystemMessageParam(role="system", content=SYSTEM_BIRTHDAY_PROMPT.strip()),
        ChatCompletionUserMessageParam(
            role="user", content=f"–°–≥–µ–Ω–µ—Ä–∏—Ä—É–π –∫—Ä–µ–∞—Ç–∏–≤–Ω–æ–µ –ø–æ–∑–¥—Ä–∞–≤–ª–µ–Ω–∏–µ —Å –¥–Ω–µ–º —Ä–æ–∂–¥–µ–Ω–∏—è –¥–ª—è {name}."
        ),
    ]

    try:
        completion = await client.chat.completions.create(
            model="gpt-5-chat",
            messages=prompt,
            temperature=0.8,  # –û–ø—Ç–∏–º–∞–ª—å–Ω—ã–π –±–∞–ª–∞–Ω—Å –∫—Ä–µ–∞—Ç–∏–≤–Ω–æ—Å—Ç–∏/–∫–æ–≥–µ—Ä–µ–Ω—Ç–Ω–æ—Å—Ç–∏
            top_p=0.8,  # –®–∏—Ä–µ –≤—ã–±–æ—Ä–∫–∞ —Å–ª–æ–≤
            frequency_penalty=0.1,  # –ü–æ–æ—â—Ä—è–µ—Ç –Ω–æ–≤—ã–µ —Ñ–æ—Ä–º—É–ª–∏—Ä–æ–≤–∫–∏
            presence_penalty=0.2,  # –ü–æ–æ—â—Ä—è–µ—Ç –Ω–æ–≤—ã–µ —Ç–µ–º—ã
            max_tokens=400,
        )
        text = completion.choices[0].message.content.strip()
        text = await clean_text(text)
        return text
    except Exception as e:
        print(f"[–û—à–∏–±–∫–∞ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –ø–æ–∑–¥—Ä–∞–≤–ª–µ–Ω–∏—è]: {e}")
        return f"–ü–æ–∑–¥—Ä–∞–≤–ª—è–µ–º {display_name} —Å –¥–Ω—ë–º —Ä–æ–∂–¥–µ–Ω–∏—è! üéâ"
